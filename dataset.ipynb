{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # ICL: GSM8K and BBH\n",
    "# gsm8k = load_dataset(\"gsm8k\", 'main')\n",
    "# print(gsm8k)\n",
    "# print(gsm8k['train'][0]['question'])\n",
    "# print(gsm8k['train'][0]['answer'], '\\n')\n",
    "\n",
    "# bbh = load_dataset(\"lukaemon/bbh\", 'boolean_expressions')\n",
    "# print(bbh)\n",
    "# print(bbh['test'][0]['input'])\n",
    "# print(bbh['test'][0]['target'], '\\n')\n",
    "\n",
    "# # Contextual Understanding: ShareGPT and Arxiv-March23\n",
    "# sharegpt = load_dataset(\"liyucheng/ShareGPT90K\")\n",
    "# print(sharegpt)\n",
    "# print(sharegpt['train'][0]['conversations']['from'])\n",
    "# print(sharegpt['train'][0]['conversations']['value'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       " 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install llmlingua datasets\n",
    "from datasets import load_dataset\n",
    "# !wget https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/gsm8k/lib_prompt/prompt_hardest.txt\n",
    "prompt_complex = open(\"./prompt_hardest.txt\").read()\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "print(gsm8k)\n",
    "gsm8k_test = gsm8k[\"test\"]\n",
    "gsm8k_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import several open-source LLMs\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "access_token = \"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\"\n",
    "# config = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\").to('cuda:1')\n",
    "# config = AutoConfig.from_pretrained(\"facebook/blenderbot-3B\") \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-3B\", config=config).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLMLingua\n",
    "# !pip install llmlingua\n",
    "from llmlingua import PromptCompressor\n",
    "llm_lingua = PromptCompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "# Try answering the first question\n",
    "question = gsm8k['train'][0]['question']\n",
    "answer = gsm8k['train'][0]['answer']\n",
    "# example = prompt_complex.split(\"\\n\\n\")[0]\n",
    "instruction = \"Please reference the following examples to answer the math question:\\n\"\n",
    "prompt = instruction + prompt_complex + \"\\n\\nQuestion: \" + question\n",
    "print(\"Question:\", question)\n",
    "# inputs = tokenizer(prompt, return_tensors='pt').to('cuda:0')\n",
    "# Greedy decoding with a temperature of 0 to improve stability\n",
    "# output = model.generate(**inputs, do_sample=False)\n",
    "# print(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device=0)\n",
    "output = pipe(prompt)\n",
    "print(\"Response:\", output[0]['generated_text'])\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import re\n",
    "\n",
    "def extract_ans(ans_model: str):\n",
    "    ans_model = ans_model.split(\"\\n\")\n",
    "    ans = []\n",
    "    residual = []\n",
    "    for li, al in enumerate(ans_model):\n",
    "        ans.append(al)\n",
    "        if \"answer is\" in al:\n",
    "            break\n",
    "    residual = list(ans_model[li + 1 :])\n",
    "    ans = \"\\n\".join(ans)\n",
    "    residual = \"\\n\".join(residual)\n",
    "    return ans, residual\n",
    "\n",
    "def parse_pred_ans(filename):\n",
    "    with open(filename) as fd:\n",
    "        lines = fd.readlines()\n",
    "    am, a = None, None\n",
    "    num_q, acc = 0, 0\n",
    "    current_mode = \"none\"\n",
    "    questions = []\n",
    "    ans_pred = []\n",
    "    ans_gold = []\n",
    "    for l in lines:\n",
    "        l = l.replace(\",\", \"\")\n",
    "        if l.startswith(\"Q: \"):\n",
    "            if am is not None and a is not None:\n",
    "                questions.append(q)\n",
    "                ans_pred.append(am)\n",
    "                ans_gold.append(a)\n",
    "                if test_answer(am, a):\n",
    "                    acc += 1\n",
    "            current_mode = \"q\"\n",
    "            q = l\n",
    "            num_q += 1\n",
    "        elif l.startswith(\"A_model:\"):\n",
    "            current_mode = \"am\"\n",
    "            am = l\n",
    "        elif l.startswith(\"A:\"):\n",
    "            current_mode = \"a\"\n",
    "            a = l\n",
    "        else:\n",
    "            if current_mode == \"q\":\n",
    "                q += l\n",
    "            elif current_mode == \"am\":\n",
    "                am += l\n",
    "            elif current_mode == \"a\":\n",
    "                a += l\n",
    "            else:\n",
    "                raise ValueError(current_mode)\n",
    "\n",
    "    questions.append(q)\n",
    "    ans_pred.append(am)\n",
    "    ans_gold.append(a)\n",
    "    if test_answer(am, a):\n",
    "        acc += 1\n",
    "    print(\"num_q %d correct %d ratio %.4f\" % (num_q, acc, float(acc / num_q)))\n",
    "    return questions, ans_pred, ans_gold\n",
    "\n",
    "\n",
    "def get_result(text: str):\n",
    "    pattern = \"\\d*\\.?\\d+\"\n",
    "    res = re.findall(pattern, text)\n",
    "    return res[-1] if res else \"\"\n",
    "\n",
    "\n",
    "def test_answer(pred_str, ans_str):\n",
    "    pred, gold = get_result(pred_str), get_result(ans_str)\n",
    "    return pred == gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in GSM8K test set\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "i = 0\n",
    "# compressed_prompt = llm_lingua.compress_prompt(\n",
    "#     prompt_complex.split(\"\\n\\n\"),\n",
    "#     instruction=\"\",\n",
    "#     question=\"\",\n",
    "#     target_token=200,\n",
    "#     context_budget=\"*1.5\",\n",
    "#     iterative_size=100,\n",
    "# )\n",
    "\n",
    "for q, a in tqdm(zip(gsm8k_test['question'], gsm8k_test['answer']), \n",
    "                           total=len(gsm8k_test['question'])):\n",
    "    # instruction = \"Please reference the following examples to answer the math question,\\n\"\n",
    "    # prompt = instruction + compressed_prompt[\"compressed_prompt\"] + \"\\n\\nQuestion: \" + q + \"\\n\"\n",
    "    # ans_model = response[\"choices\"][0][\"text\"]\n",
    "    inputs = tokenizer(q, return_tensors='pt', truncation=True, max_length=128).to('cuda:1')\n",
    "    response = model.generate(**inputs, temperature=0, max_length=1000)\n",
    "    ans_model = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ans_, residual = extract_ans(ans_model)\n",
    "    # with open('outputs/test_gpt_3.5_turbo_LLMLingua_174.txt', 'a') as fd:\n",
    "    with open('outputs/test_blenderbot_3B.txt', 'a') as fd:\n",
    "        fd.write(\"Q: %s\\nA_model:\\n%s\\nA:\\n%s\\n\\n\" % (q, ans_.replace(\"Q:\", \"\").replace(\"A:\", \"\"), a))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'conversations'],\n",
       "        num_rows: 90665\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sharegpt = load_dataset(\"liyucheng/ShareGPT90K\")\n",
    "sharegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"root@openvpn:/home/openvpn# ./openvpn-install.sh\\nWelcome to OpenVPN-install!\\nThe git repository is available at: https://github.com/angristan/openvpn-install\\n\\nIt looks like OpenVPN is already installed.\\n\\nWhat do you want to do?\\n   1) Add a new user\\n   2) Revoke existing user\\n   3) Remove OpenVPN\\n   4) Exit\\nSelect an option [1-4]: 1\\n\\nTell me a name for the client.\\nThe name must consist of alphanumeric character. It may also include an underscore or a dash.\\nClient name: naam\\n\\nDo you want to protect the configuration file with a password?\\n(e.g. encrypt the private key with a password)\\n   1) Add a passwordless client\\n   2) Use a password for the client\\nSelect an option [1-2]: 1\\n\\nNote: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars\\nUsing SSL: openssl OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022)\\n-----\\nUsing configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-54848.BT2FXv/tmp.dFLd6V\\nEnter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key:\\nCheck that the request matches the signature\\nSignature ok\\nThe Subject's Distinguished Name is as follows\\ncommonName            :ASN.1 12:'naam'\\nCertificate is to be certified until Apr 21 22:48:57 2025 GMT (825 days)\\n\\nWrite out database with 1 new entries\\nData Base Updated\\n\\nClient naam added.\\n\\nThe configuration file has been written to /home/openvpn/naam.ovpn.\\nDownload the .ovpn file and import it in your OpenVPN client.\\nroot@openvpn:/home/openvpn# \\n\\n\\nkan je op basis van deze gegevens een verslag maken hoe je iemand toevoegd op een openvpn server\",\n",
       " '<div class=\"markdown prose w-full break-words dark:prose-invert dark\"><p>Op basis van de gegevens in de prompt log kan worden opgemerkt dat een script genaamd \"openvpn-install.sh\" wordt uitgevoerd op een OpenVPN server. Het script lijkt te controleren of OpenVPN al is geïnstalleerd op de server en geeft vervolgens de gebruiker de optie om een nieuwe gebruiker toe te voegen, een bestaande gebruiker te intrekken, OpenVPN te verwijderen of het script te verlaten.</p><p>De gebruiker kiest vervolg</p></div>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharegpt['train'][0]['conversations']['value'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
