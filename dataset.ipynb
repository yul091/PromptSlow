{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # ICL: GSM8K and BBH\n",
    "# gsm8k = load_dataset(\"gsm8k\", 'main')\n",
    "# print(gsm8k)\n",
    "# print(gsm8k['train'][0]['question'])\n",
    "# print(gsm8k['train'][0]['answer'], '\\n')\n",
    "\n",
    "# bbh = load_dataset(\"lukaemon/bbh\", 'boolean_expressions')\n",
    "# print(bbh)\n",
    "# print(bbh['test'][0]['input'])\n",
    "# print(bbh['test'][0]['target'], '\\n')\n",
    "\n",
    "# # Contextual Understanding: ShareGPT and Arxiv-March23\n",
    "# sharegpt = load_dataset(\"liyucheng/ShareGPT90K\")\n",
    "# print(sharegpt)\n",
    "# print(sharegpt['train'][0]['conversations']['from'])\n",
    "# print(sharegpt['train'][0]['conversations']['value'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llmlingua datasets\n",
    "from datasets import load_dataset\n",
    "# !wget https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/gsm8k/lib_prompt/prompt_hardest.txt\n",
    "prompt_complex = open(\"./prompt_hardest.txt\").read()\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "print(gsm8k)\n",
    "gsm8k_test = gsm8k[\"test\"]\n",
    "gsm8k_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_test[:2]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import several open-source LLMs\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "access_token = \"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\"\n",
    "# config = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\").to('cuda:1')\n",
    "# config = AutoConfig.from_pretrained(\"facebook/blenderbot-3B\") \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-3B\", config=config).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLMLingua\n",
    "# !pip install llmlingua\n",
    "from llmlingua import PromptCompressor\n",
    "llm_lingua = PromptCompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "# Try answering the first question\n",
    "question = gsm8k['train'][0]['question']\n",
    "answer = gsm8k['train'][0]['answer']\n",
    "# example = prompt_complex.split(\"\\n\\n\")[0]\n",
    "instruction = \"Please reference the following examples to answer the math question:\\n\"\n",
    "prompt = instruction + prompt_complex + \"\\n\\nQuestion: \" + question\n",
    "print(\"Question:\", question)\n",
    "# inputs = tokenizer(prompt, return_tensors='pt').to('cuda:0')\n",
    "# Greedy decoding with a temperature of 0 to improve stability\n",
    "# output = model.generate(**inputs, do_sample=False)\n",
    "# print(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device=0)\n",
    "output = pipe(prompt)\n",
    "print(\"Response:\", output[0]['generated_text'])\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import re\n",
    "\n",
    "def extract_ans(ans_model: str):\n",
    "    ans_model = ans_model.split(\"\\n\")\n",
    "    ans = []\n",
    "    residual = []\n",
    "    for li, al in enumerate(ans_model):\n",
    "        ans.append(al)\n",
    "        if \"answer is\" in al:\n",
    "            break\n",
    "    residual = list(ans_model[li + 1 :])\n",
    "    ans = \"\\n\".join(ans)\n",
    "    residual = \"\\n\".join(residual)\n",
    "    return ans, residual\n",
    "\n",
    "def parse_pred_ans(filename):\n",
    "    with open(filename) as fd:\n",
    "        lines = fd.readlines()\n",
    "    am, a = None, None\n",
    "    num_q, acc = 0, 0\n",
    "    current_mode = \"none\"\n",
    "    questions = []\n",
    "    ans_pred = []\n",
    "    ans_gold = []\n",
    "    for l in lines:\n",
    "        l = l.replace(\",\", \"\")\n",
    "        if l.startswith(\"Q: \"):\n",
    "            if am is not None and a is not None:\n",
    "                questions.append(q)\n",
    "                ans_pred.append(am)\n",
    "                ans_gold.append(a)\n",
    "                if test_answer(am, a):\n",
    "                    acc += 1\n",
    "            current_mode = \"q\"\n",
    "            q = l\n",
    "            num_q += 1\n",
    "        elif l.startswith(\"A_model:\"):\n",
    "            current_mode = \"am\"\n",
    "            am = l\n",
    "        elif l.startswith(\"A:\"):\n",
    "            current_mode = \"a\"\n",
    "            a = l\n",
    "        else:\n",
    "            if current_mode == \"q\":\n",
    "                q += l\n",
    "            elif current_mode == \"am\":\n",
    "                am += l\n",
    "            elif current_mode == \"a\":\n",
    "                a += l\n",
    "            else:\n",
    "                raise ValueError(current_mode)\n",
    "\n",
    "    questions.append(q)\n",
    "    ans_pred.append(am)\n",
    "    ans_gold.append(a)\n",
    "    if test_answer(am, a):\n",
    "        acc += 1\n",
    "    print(\"num_q %d correct %d ratio %.4f\" % (num_q, acc, float(acc / num_q)))\n",
    "    return questions, ans_pred, ans_gold\n",
    "\n",
    "\n",
    "def get_result(text: str):\n",
    "    pattern = \"\\d*\\.?\\d+\"\n",
    "    res = re.findall(pattern, text)\n",
    "    return res[-1] if res else \"\"\n",
    "\n",
    "\n",
    "def test_answer(pred_str, ans_str):\n",
    "    pred, gold = get_result(pred_str), get_result(ans_str)\n",
    "    return pred == gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in GSM8K test set\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "i = 0\n",
    "# compressed_prompt = llm_lingua.compress_prompt(\n",
    "#     prompt_complex.split(\"\\n\\n\"),\n",
    "#     instruction=\"\",\n",
    "#     question=\"\",\n",
    "#     target_token=200,\n",
    "#     context_budget=\"*1.5\",\n",
    "#     iterative_size=100,\n",
    "# )\n",
    "\n",
    "for q, a in tqdm(zip(gsm8k_test['question'], gsm8k_test['answer']), \n",
    "                           total=len(gsm8k_test['question'])):\n",
    "    # instruction = \"Please reference the following examples to answer the math question,\\n\"\n",
    "    # prompt = instruction + compressed_prompt[\"compressed_prompt\"] + \"\\n\\nQuestion: \" + q + \"\\n\"\n",
    "    # ans_model = response[\"choices\"][0][\"text\"]\n",
    "    inputs = tokenizer(q, return_tensors='pt', truncation=True, max_length=128).to('cuda:1')\n",
    "    response = model.generate(**inputs, temperature=0, max_length=1000)\n",
    "    ans_model = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ans_, residual = extract_ans(ans_model)\n",
    "    # with open('outputs/test_gpt_3.5_turbo_LLMLingua_174.txt', 'a') as fd:\n",
    "    with open('outputs/test_blenderbot_3B.txt', 'a') as fd:\n",
    "        fd.write(\"Q: %s\\nA_model:\\n%s\\nA:\\n%s\\n\\n\" % (q, ans_.replace(\"Q:\", \"\").replace(\"A:\", \"\"), a))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'chat'],\n",
       "        num_rows: 575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sharegpt = load_dataset(\"liyucheng/sharegpt-500\")\n",
    "sharegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'reference'],\n",
      "        num_rows: 90665\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': '百大青農輔導計畫：為協助青年從農，盤點本會各項產業輔導措施，建立資源整合輔導平臺，提供一站式之專案輔導服務，協助青年農民穩健經營，逐步擴大規模，成為專業生產供貨者，或跳脫純粹生產面，而成為具規模之農產行銷、加工經營者，或成為其他創新加值發展之農業產業價值鏈經營人員，打造出青年從農之發展標竿，並串聯整合其他青年農民與上下游農業工作者，帶動地區產業與其他青農共同成長，一起活絡農村，發展出創新的農業產業價值鏈，讓「農」業的生產、生活與生態價值能充分發揮，使農業能升級且具競爭力，成為值得傳承的事業。',\n",
       " 'reference': '<div><p>百大青農輔導計畫旨在協助青年農民穩健經營，逐步擴大規模，成為專業生產供貨者，或跳脫純粹生產面，而成為具規模之農產行銷、加工經營者，或成為其他創新加值發展之農業產業價值鏈經營人員，藉此打造出青年從農之發展標竿。本計畫將建立資源整合輔導平臺，提供一站式之專案輔導服務，協助青年農民了解本會各項產業輔導措施，進而有效運用相關資源，提升生產力與市場競爭力。</p><p>此外，本計畫也將串聯整合其他青年農民與上下游農業工作者，協助青年農民瞭解整個農業產業價值鏈的運作，從而創造更多的商機。本計畫也期望能帶動地區產業與其他青農共同成長，一起活絡農村，發展出創新的農業產業價值鏈，使農業的生產、生活與生態價值能充分發揮，進而升級且具競爭力，成為值得傳承的事業。</p></div>'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Dataset import prompt_dataset\n",
    "\n",
    "task = 'conversational'\n",
    "instruction, demonstrations, prefix, train_dataset, test_dataset = prompt_dataset(task=task)\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
