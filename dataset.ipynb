{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # ICL: GSM8K and BBH\n",
    "# gsm8k = load_dataset(\"gsm8k\", 'main')\n",
    "# print(gsm8k)\n",
    "# print(gsm8k['train'][0]['question'])\n",
    "# print(gsm8k['train'][0]['answer'], '\\n')\n",
    "\n",
    "# bbh = load_dataset(\"lukaemon/bbh\", 'boolean_expressions')\n",
    "# print(bbh)\n",
    "# print(bbh['test'][0]['input'])\n",
    "# print(bbh['test'][0]['target'], '\\n')\n",
    "\n",
    "# # Contextual Understanding: ShareGPT and Arxiv-March23\n",
    "# sharegpt = load_dataset(\"liyucheng/ShareGPT90K\")\n",
    "# print(sharegpt)\n",
    "# print(sharegpt['train'][0]['conversations']['from'])\n",
    "# print(sharegpt['train'][0]['conversations']['value'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llmlingua datasets\n",
    "from datasets import load_dataset\n",
    "# !wget https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/gsm8k/lib_prompt/prompt_hardest.txt\n",
    "prompt_complex = open(\"./prompt_hardest.txt\").read()\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "print(gsm8k)\n",
    "gsm8k_test = gsm8k[\"test\"]\n",
    "gsm8k_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import several open-source LLMs\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "access_token = \"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\"\n",
    "# config = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\").to('cuda:1')\n",
    "# config = AutoConfig.from_pretrained(\"facebook/blenderbot-3B\") \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-3B\", config=config).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLMLingua\n",
    "# !pip install llmlingua\n",
    "from llmlingua import PromptCompressor\n",
    "llm_lingua = PromptCompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "# Try answering the first question\n",
    "question = gsm8k['train'][0]['question']\n",
    "answer = gsm8k['train'][0]['answer']\n",
    "# example = prompt_complex.split(\"\\n\\n\")[0]\n",
    "instruction = \"Please reference the following examples to answer the math question:\\n\"\n",
    "prompt = instruction + prompt_complex + \"\\n\\nQuestion: \" + question\n",
    "print(\"Question:\", question)\n",
    "# inputs = tokenizer(prompt, return_tensors='pt').to('cuda:0')\n",
    "# Greedy decoding with a temperature of 0 to improve stability\n",
    "# output = model.generate(**inputs, do_sample=False)\n",
    "# print(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device=0)\n",
    "output = pipe(prompt)\n",
    "print(\"Response:\", output[0]['generated_text'])\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import re\n",
    "\n",
    "def extract_ans(ans_model: str):\n",
    "    ans_model = ans_model.split(\"\\n\")\n",
    "    ans = []\n",
    "    residual = []\n",
    "    for li, al in enumerate(ans_model):\n",
    "        ans.append(al)\n",
    "        if \"answer is\" in al:\n",
    "            break\n",
    "    residual = list(ans_model[li + 1 :])\n",
    "    ans = \"\\n\".join(ans)\n",
    "    residual = \"\\n\".join(residual)\n",
    "    return ans, residual\n",
    "\n",
    "def parse_pred_ans(filename):\n",
    "    with open(filename) as fd:\n",
    "        lines = fd.readlines()\n",
    "    am, a = None, None\n",
    "    num_q, acc = 0, 0\n",
    "    current_mode = \"none\"\n",
    "    questions = []\n",
    "    ans_pred = []\n",
    "    ans_gold = []\n",
    "    for l in lines:\n",
    "        l = l.replace(\",\", \"\")\n",
    "        if l.startswith(\"Q: \"):\n",
    "            if am is not None and a is not None:\n",
    "                questions.append(q)\n",
    "                ans_pred.append(am)\n",
    "                ans_gold.append(a)\n",
    "                if test_answer(am, a):\n",
    "                    acc += 1\n",
    "            current_mode = \"q\"\n",
    "            q = l\n",
    "            num_q += 1\n",
    "        elif l.startswith(\"A_model:\"):\n",
    "            current_mode = \"am\"\n",
    "            am = l\n",
    "        elif l.startswith(\"A:\"):\n",
    "            current_mode = \"a\"\n",
    "            a = l\n",
    "        else:\n",
    "            if current_mode == \"q\":\n",
    "                q += l\n",
    "            elif current_mode == \"am\":\n",
    "                am += l\n",
    "            elif current_mode == \"a\":\n",
    "                a += l\n",
    "            else:\n",
    "                raise ValueError(current_mode)\n",
    "\n",
    "    questions.append(q)\n",
    "    ans_pred.append(am)\n",
    "    ans_gold.append(a)\n",
    "    if test_answer(am, a):\n",
    "        acc += 1\n",
    "    print(\"num_q %d correct %d ratio %.4f\" % (num_q, acc, float(acc / num_q)))\n",
    "    return questions, ans_pred, ans_gold\n",
    "\n",
    "\n",
    "def get_result(text: str):\n",
    "    pattern = \"\\d*\\.?\\d+\"\n",
    "    res = re.findall(pattern, text)\n",
    "    return res[-1] if res else \"\"\n",
    "\n",
    "\n",
    "def test_answer(pred_str, ans_str):\n",
    "    pred, gold = get_result(pred_str), get_result(ans_str)\n",
    "    return pred == gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in GSM8K test set\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "i = 0\n",
    "# compressed_prompt = llm_lingua.compress_prompt(\n",
    "#     prompt_complex.split(\"\\n\\n\"),\n",
    "#     instruction=\"\",\n",
    "#     question=\"\",\n",
    "#     target_token=200,\n",
    "#     context_budget=\"*1.5\",\n",
    "#     iterative_size=100,\n",
    "# )\n",
    "\n",
    "for q, a in tqdm(zip(gsm8k_test['question'], gsm8k_test['answer']), \n",
    "                           total=len(gsm8k_test['question'])):\n",
    "    # instruction = \"Please reference the following examples to answer the math question,\\n\"\n",
    "    # prompt = instruction + compressed_prompt[\"compressed_prompt\"] + \"\\n\\nQuestion: \" + q + \"\\n\"\n",
    "    # ans_model = response[\"choices\"][0][\"text\"]\n",
    "    inputs = tokenizer(q, return_tensors='pt', truncation=True, max_length=128).to('cuda:1')\n",
    "    response = model.generate(**inputs, temperature=0, max_length=1000)\n",
    "    ans_model = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ans_, residual = extract_ans(ans_model)\n",
    "    # with open('outputs/test_gpt_3.5_turbo_LLMLingua_174.txt', 'a') as fd:\n",
    "    with open('outputs/test_blenderbot_3B.txt', 'a') as fd:\n",
    "        fd.write(\"Q: %s\\nA_model:\\n%s\\nA:\\n%s\\n\\n\" % (q, ans_.replace(\"Q:\", \"\").replace(\"A:\", \"\"), a))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yli927/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'conversations'],\n",
       "        num_rows: 90665\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sharegpt = load_dataset(\"liyucheng/ShareGPT90K\")\n",
    "sharegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "### Installatie\n",
      "\n",
      "Deze module kan worden geïnstalleerd via de opdracht `composer require cocopon/translatable`\n",
      "\n",
      "### Inleiding\n",
      "\n",
      "Deze module is ontworpen om het makkelijker te maken om een translatable component te maken.\n",
      "\n",
      "### Hoe werkt deze module?\n",
      "\n",
      "Deze module gebruikt een `Translatable` interface. Deze interface moet worden geïmplementeerd door de component.\n",
      "\n",
      "### Voorbeeld\n",
      "\n",
      "```php\n",
      "use Cocopon\\Translatable;\n",
      "\n",
      "class TestComponent extends Translatable\n",
      "{\n",
      "    public function getTranslation(): string\n",
      "    {\n",
      "        return $this->getTranslation('en');\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### Hoe werkt de interface?\n",
      "\n",
      "Deze interface geeft de component de mogelijkheid om de vertalingen van de component in te stellen. De vertalingen worden opgeslagen in een `TranslationBag` die de component kan gebruiken.\n",
      "\n",
      "### Hoe gebruik ik deze module?\n",
      "\n",
      "Deze module kan worden gebruikt als\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "chatbot = pipeline(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    # tokenizer=\"meta-llama/Llama-2-7b-hf\",\n",
    "    task=\"conversational\",\n",
    "    device=0,\n",
    ")\n",
    "\n",
    "instance = sharegpt['train'][0]\n",
    "conversation = Conversation(instance['conversations']['value'][0])\n",
    "# print(instance['conversations']['value'][0])\n",
    "\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation.messages[-1][\"content\"])\n",
    "\n",
    "conversation.add_message({\"role\": \"user\", \"content\": instance['conversations']['value'][2]})\n",
    "conversation = chatbot(conversation)\n",
    "print(conversation.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
