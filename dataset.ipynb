{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # ICL: GSM8K and BBH\n",
    "# gsm8k = load_dataset(\"gsm8k\", 'main')\n",
    "# print(gsm8k)\n",
    "# print(gsm8k['train'][0]['question'])\n",
    "# print(gsm8k['train'][0]['answer'], '\\n')\n",
    "\n",
    "# bbh = load_dataset(\"lukaemon/bbh\", 'boolean_expressions')\n",
    "# print(bbh)\n",
    "# print(bbh['test'][0]['input'])\n",
    "# print(bbh['test'][0]['target'], '\\n')\n",
    "\n",
    "# # Contextual Understanding: ShareGPT and Arxiv-March23\n",
    "# sharegpt = load_dataset(\"liyucheng/ShareGPT90K\")\n",
    "# print(sharegpt)\n",
    "# print(sharegpt['train'][0]['conversations']['from'])\n",
    "# print(sharegpt['train'][0]['conversations']['value'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yli927/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
       " 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install llmlingua datasets\n",
    "from datasets import load_dataset\n",
    "# !wget https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/gsm8k/lib_prompt/prompt_hardest.txt\n",
    "prompt_complex = open(\"./prompt_hardest.txt\").read()\n",
    "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "print(gsm8k)\n",
    "gsm8k_test = gsm8k[\"test\"]\n",
    "gsm8k_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_test[:2]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import several open-source LLMs\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "\n",
    "# access_token = \"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\"\n",
    "# config = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "# model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf', token=access_token)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\").to('cuda:1')\n",
    "# config = AutoConfig.from_pretrained(\"facebook/blenderbot-3B\") \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-3B\", config=config).to('cuda:1')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['Hello, my dog is cute', 'Hello, my cat is cute', 'This is a sentence.']\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "outputs = model.generate(**inputs, max_length=50, do_sample=True) # shape: (B x T)\n",
    "responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LLMLingua\n",
    "# !pip install llmlingua\n",
    "from llmlingua import PromptCompressor\n",
    "llm_lingua = PromptCompressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "# Try answering the first question\n",
    "question = gsm8k['train'][0]['question']\n",
    "answer = gsm8k['train'][0]['answer']\n",
    "# example = prompt_complex.split(\"\\n\\n\")[0]\n",
    "instruction = \"Please reference the following examples to answer the math question:\\n\"\n",
    "prompt = instruction + prompt_complex + \"\\n\\nQuestion: \" + question\n",
    "print(\"Question:\", question)\n",
    "# inputs = tokenizer(prompt, return_tensors='pt').to('cuda:0')\n",
    "# Greedy decoding with a temperature of 0 to improve stability\n",
    "# output = model.generate(**inputs, do_sample=False)\n",
    "# print(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device=0)\n",
    "output = pipe(prompt)\n",
    "print(\"Response:\", output[0]['generated_text'])\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import re\n",
    "\n",
    "def extract_ans(ans_model: str):\n",
    "    ans_model = ans_model.split(\"\\n\")\n",
    "    ans = []\n",
    "    residual = []\n",
    "    for li, al in enumerate(ans_model):\n",
    "        ans.append(al)\n",
    "        if \"answer is\" in al:\n",
    "            break\n",
    "    residual = list(ans_model[li + 1 :])\n",
    "    ans = \"\\n\".join(ans)\n",
    "    residual = \"\\n\".join(residual)\n",
    "    return ans, residual\n",
    "\n",
    "def parse_pred_ans(filename):\n",
    "    with open(filename) as fd:\n",
    "        lines = fd.readlines()\n",
    "    am, a = None, None\n",
    "    num_q, acc = 0, 0\n",
    "    current_mode = \"none\"\n",
    "    questions = []\n",
    "    ans_pred = []\n",
    "    ans_gold = []\n",
    "    for l in lines:\n",
    "        l = l.replace(\",\", \"\")\n",
    "        if l.startswith(\"Q: \"):\n",
    "            if am is not None and a is not None:\n",
    "                questions.append(q)\n",
    "                ans_pred.append(am)\n",
    "                ans_gold.append(a)\n",
    "                if test_answer(am, a):\n",
    "                    acc += 1\n",
    "            current_mode = \"q\"\n",
    "            q = l\n",
    "            num_q += 1\n",
    "        elif l.startswith(\"A_model:\"):\n",
    "            current_mode = \"am\"\n",
    "            am = l\n",
    "        elif l.startswith(\"A:\"):\n",
    "            current_mode = \"a\"\n",
    "            a = l\n",
    "        else:\n",
    "            if current_mode == \"q\":\n",
    "                q += l\n",
    "            elif current_mode == \"am\":\n",
    "                am += l\n",
    "            elif current_mode == \"a\":\n",
    "                a += l\n",
    "            else:\n",
    "                raise ValueError(current_mode)\n",
    "\n",
    "    questions.append(q)\n",
    "    ans_pred.append(am)\n",
    "    ans_gold.append(a)\n",
    "    if test_answer(am, a):\n",
    "        acc += 1\n",
    "    print(\"num_q %d correct %d ratio %.4f\" % (num_q, acc, float(acc / num_q)))\n",
    "    return questions, ans_pred, ans_gold\n",
    "\n",
    "\n",
    "def get_result(text: str):\n",
    "    pattern = \"\\d*\\.?\\d+\"\n",
    "    res = re.findall(pattern, text)\n",
    "    return res[-1] if res else \"\"\n",
    "\n",
    "\n",
    "def test_answer(pred_str, ans_str):\n",
    "    pred, gold = get_result(pred_str), get_result(ans_str)\n",
    "    return pred == gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test in GSM8K test set\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "i = 0\n",
    "# compressed_prompt = llm_lingua.compress_prompt(\n",
    "#     prompt_complex.split(\"\\n\\n\"),\n",
    "#     instruction=\"\",\n",
    "#     question=\"\",\n",
    "#     target_token=200,\n",
    "#     context_budget=\"*1.5\",\n",
    "#     iterative_size=100,\n",
    "# )\n",
    "\n",
    "for q, a in tqdm(zip(gsm8k_test['question'], gsm8k_test['answer']), \n",
    "                           total=len(gsm8k_test['question'])):\n",
    "    # instruction = \"Please reference the following examples to answer the math question,\\n\"\n",
    "    # prompt = instruction + compressed_prompt[\"compressed_prompt\"] + \"\\n\\nQuestion: \" + q + \"\\n\"\n",
    "    # ans_model = response[\"choices\"][0][\"text\"]\n",
    "    inputs = tokenizer(q, return_tensors='pt', truncation=True, max_length=128).to('cuda:1')\n",
    "    response = model.generate(**inputs, temperature=0, max_length=1000)\n",
    "    ans_model = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    ans_, residual = extract_ans(ans_model)\n",
    "    # with open('outputs/test_gpt_3.5_turbo_LLMLingua_174.txt', 'a') as fd:\n",
    "    with open('outputs/test_blenderbot_3B.txt', 'a') as fd:\n",
    "        fd.write(\"Q: %s\\nA_model:\\n%s\\nA:\\n%s\\n\\n\" % (q, ans_.replace(\"Q:\", \"\").replace(\"A:\", \"\"), a))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShareGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'chat'],\n",
       "        num_rows: 575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sharegpt = load_dataset(\"liyucheng/sharegpt-500\")\n",
    "sharegpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'reference'],\n",
      "        num_rows: 975\n",
      "    })\n",
      "})\n",
      "{'query': 'design a circuit to play FizzBuzz using 7400-series logic gates and 7-segment displays', 'reference': '<div><p>FizzBuzz is a well-known coding challenge where you have to print out the numbers from 1 to 100, but for multiples of three, you print \"Fizz\" instead of the number, and for multiples of five, you print \"Buzz.\" For numbers that are multiples of both three and five, you print \"FizzBuzz.\"</p><p>To design a circuit that plays FizzBuzz using 7400-series logic gates and 7-segment displays, you would need to break down the problem into smaller components. Here\\'s one possible approach:</p><ol><li><p>Create a counter that counts from 1 to 100. You can use a 4-bit binary counter such as the 74161, which has a synchronous reset input that you can use to reset the counter to 1.</p></li><li><p>Use logic gates to determine whether the current count is a multiple of three, five, or both. For example, you can use a combination of AND gates and comparators to check if the count is divisible by 3 or 5. If the count is divisible by both 3 and 5, then it is a multiple of 15 (i.e., FizzBuzz).</p></li><li><p>Use a decoder such as the 7447 or 74148 to convert the binary output of the counter into a seven-segment display output. You will need to connect the decoder to a 7-segment display, which can display the numbers 0-9 as well as some letters.</p></li><li><p>Use additional logic gates to control which output is displayed on the 7-segment display based on whether the current count is a multiple of 3, 5, or both. For example, if the count is a multiple of 3, you can display \"F\" on the leftmost segment of the display, and if it is a multiple of 5, you can display \"B\" on the rightmost segment. If it is a multiple of both, you can display \"FB\" on the display.</p></li></ol><p>Here\\'s a high-level diagram of what the circuit might look like:</p><pre><div class=\"bg-black rounded-md mb-4\"><div class=\"flex items-center relative text-gray-200 bg-gray-800 px-4 py-2 text-xs font-sans\"><span>lua</span><svg stroke=\"currentColor\" fill=\"none\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"h-4 w-4\" height=\"1em\" width=\"1em\" xmlns=\"http://www.w3.org/2000/svg\"></svg>Copy code</div><div><code class=\"!whitespace-pre hljs language-lua\">+<span class=\"hljs-comment\">-------+      +-----+</span>\\n| <span class=\"hljs-number\">74161</span> |<span class=\"hljs-comment\">------| AND |-------+</span>\\n+<span class=\"hljs-comment\">-------+      +-----+       |</span>\\n                 |           |\\n+<span class=\"hljs-comment\">-------+      +-----+    +-----+</span>\\n| <span class=\"hljs-number\">74161</span> |<span class=\"hljs-comment\">------| AND |----| AND |---+</span>\\n+<span class=\"hljs-comment\">-------+      +-----+    +-----+   |</span>\\n                                     |\\n+<span class=\"hljs-comment\">-------+      +-----+             +-+-+</span>\\n| <span class=\"hljs-number\">74161</span> |<span class=\"hljs-comment\">------| AND |-------------|   |  7-segment</span>\\n+<span class=\"hljs-comment\">-------+      +-----+             +-+-+  display</span>\\n                 |                    |\\n+<span class=\"hljs-comment\">-------+      +-----+             +-+-+</span>\\n| <span class=\"hljs-number\">74161</span> |<span class=\"hljs-comment\">------| AND |-------------|   |</span>\\n+<span class=\"hljs-comment\">-------+      +-----+             +-+-+</span>\\n                                     |\\n+<span class=\"hljs-comment\">-------+      +-----+    +-----+   |</span>\\n| <span class=\"hljs-number\">74161</span> |<span class=\"hljs-comment\">------| AND |----| AND |---+</span>\\n+<span class=\"hljs-comment\">-------+      +-----+    +-----+   |</span>\\n                 |           |       |\\n+<span class=\"hljs-comment\">-------+      +-----+    +-----+   |</span>\\n| <span class=\"hljs-number\">74161</span> |<span class=\"hljs-comment\">------| AND |----| AND |---+</span>\\n+<span class=\"hljs-comment\">-------+      +-----+    +-----+   |</span>\\n                                     |\\n+<span class=\"hljs-comment\">-------+      +-----+             +-+-+</span>\\n| <span class=\"hljs-number\">74161</span> |<span class=\"hljs-comment\">------| AND |-------------|   |</span>\\n+<span class=\"hljs-comment\">-------+      +-----+             +-+-+</span>\\n\\n</code></div></div></pre><p>In this diagram, the 74161 counter is connected to three AND gates, each of which checks whether the current count is a multiple of 3, 5, or 15 (i.e., both 3 and 5). The outputs of these AND gates are connected to additional AND gates, which control which output</p></div>'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  3,   2,  58,  ...,   0,   0,   0],\n",
       "        [220,   3,  87,  ...,   0,   0,   0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[   3,    2, 8481,  ...,    0,    0,    0],\n",
       "        [   3,    2, 8481,  ...,    0,    0,    0]]), 'decoder_input_ids': tensor([[0, 3, 2,  ..., 0, 0, 0],\n",
       "        [0, 3, 2,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "from Dataset import prompt_dataset, get_dataloader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "task = 'conversational'\n",
    "instruction, demonstrations, prefix, train_dataset, test_dataset = prompt_dataset(task=task)\n",
    "print(test_dataset[0])\n",
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "train_dataloader = get_dataloader(\n",
    "    train_dataset, \n",
    "    tokenizer, \n",
    "    batch_size=2,\n",
    "    padding=True,\n",
    "    ignore_pad_token_for_loss=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-large-cased\")\n",
    "print(model.num_parameters())\n",
    "\n",
    "model = AutoModel.from_pretrained(\"gpt2-medium\")\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root@openvpn:/home/openvpn# ./openvpn-install.sh\n",
      "Welcome to OpenVPN-install!\n",
      "The git repository is available at: https://github.com/angristan/openvpn-install\n",
      "\n",
      "It looks like OpenVPN is already installed.\n",
      "\n",
      "What do you want to do?\n",
      "   1) Add a new user\n",
      "   2) Revoke existing user\n",
      "   3) Remove OpenVPN\n",
      "   4) Exit\n",
      "Select an option [1-4]: 1\n",
      "\n",
      "Tell me a name for the client.\n",
      "The name must consist of alphanumeric character. It may also include an underscore or a dash.\n",
      "Client name: naam\n",
      "\n",
      "Do you want to protect the configuration file with a password?\n",
      "(e.g. encrypt the private key with a password)\n",
      "   1) Add a passwordless client\n",
      "   2) Use a password for the client\n",
      "Select an option [1-2]: 1\n",
      "\n",
      "Note: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/vars\n",
      "Using SSL: openssl OpenSSL 3.0.2 15 Mar 2022 (Library: OpenSSL 3.0.2 15 Mar 2022)\n",
      "-----\n",
      "Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-54848.BT2FXv/tmp.dFLd6V\n",
      "Enter pass phrase for /etc/openvpn/easy-rsa/pki/private/ca.key:\n",
      "Check that the request matches the signature\n",
      "Signature ok\n",
      "The Subject's Distinguished Name is as follows\n",
      "commonName            :ASN.1 12:'naam'\n",
      "Certificate is to be certified until Apr 21 22:48:57 2025 GMT (825 days)\n",
      "\n",
      "Write out database with 1 new entries\n",
      "Data Base Updated\n",
      "\n",
      "Client naam added.\n",
      "\n",
      "The configuration file has been written to /home/openvpn/naam.ovpn.\n",
      "Download the .ovpn file and import it in your OpenVPN client.\n",
      "root@openvpn:/home/openvpn# \n",
      "\n",
      "\n",
      "kan je op basis van deze gegevens een verslag maken hoe je iemand toevoegd op een openvpn server\n"
     ]
    }
   ],
   "source": [
    "from helper import tokens_to_sentences\n",
    "query = train_dataset[0]['query']\n",
    "print(query)\n",
    "tokens = tokenizer.tokenize(query)\n",
    "# print(tokenizer.convert_tokens_to_string(tokens))\n",
    "# tokens_to_sentences(tokenizer.tokenize(query), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
